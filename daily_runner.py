"""
============================================================
END NF ì½˜í…ì¸  ì‹œìŠ¤í…œ - ì¼ì¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (2ë‹¨ê³„ í•µì‹¬)
============================================================
ìš”ì¼ë³„ë¡œ ì ì ˆí•œ ìˆ˜ì§‘ê¸°ë¥¼ ìë™ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ í†µí•©/ì •ê·œí™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python daily_runner.py                  # ì˜¤ëŠ˜ ìš”ì¼ì— ë§ê²Œ ìë™ ì‹¤í–‰
    python daily_runner.py --day mon        # íŠ¹ì • ìš”ì¼ ì‹¤í–‰
    python daily_runner.py --day all        # ì „ì²´ ìš”ì¼ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸)
    python daily_runner.py --dry-run        # ìˆ˜ì§‘ ì—†ì´ ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥

í™˜ê²½ë³€ìˆ˜:
    NCBI_API_KEY: PubMed API í‚¤ (ì„ íƒ)
    TELEGRAM_BOT_TOKEN: í…”ë ˆê·¸ë¨ ë´‡ í† í° (5ë‹¨ê³„)
    TELEGRAM_CHAT_ID: í…”ë ˆê·¸ë¨ ì±„íŒ… ID (5ë‹¨ê³„)
    ANTHROPIC_API_KEY: Claude API í‚¤ (3ë‹¨ê³„)
"""

import os
import sys
import json
import logging
import hashlib
import traceback
from datetime import datetime, timedelta
from pathlib import Path
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from pubmed_fetcher import PubMedFetcher
from news_fetcher import NewsFetcher
from clinical_trials_fetcher import ClinicalTrialsFetcher
from patient_story_fetcher import PatientStoryFetcher

# â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€
LOG_DIR = os.path.join(os.path.dirname(__file__), "logs")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
ARCHIVE_DIR = os.path.join(DATA_DIR, "archive")
DEDUP_FILE = os.path.join(DATA_DIR, "seen_items.json")

os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(ARCHIVE_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(
            os.path.join(LOG_DIR, f"daily_{datetime.now().strftime('%Y%m%d')}.log"),
            encoding="utf-8",
        ),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger("endnf")


# â”€â”€ ìš”ì¼ë³„ ì‹¤í–‰ ê³„íš â”€â”€
DAY_PLANS = {
    "mon": {
        "title": "ğŸ“š ì›”ìš”ì¼: NF ê´€ë ¨ ìµœì‹  êµ­ì œ ë…¼ë¬¸/ì—°êµ¬",
        "tasks": [
            {"type": "pubmed", "params": {"days_back": 7, "max_per_query": 5}},
            {"type": "news", "params": {"categories": ["general", "rarenote", "ctf"]}},
        ],
    },
    "tue": {
        "title": "ğŸ’› í™”ìš”ì¼: í™˜ì/ê°€ì¡± ì‘ì› ë©”ì‹œì§€ + ê°ë™ ì´ì•¼ê¸°",
        "tasks": [
            {"type": "patient_stories", "params": {"sources": ["reddit", "ctf"]}},
            {"type": "news", "params": {"categories": ["rarenote"]}},
        ],
    },
    "wed": {
        "title": "ğŸŒ ìˆ˜ìš”ì¼: í•´ì™¸ NF ì»¤ë®¤ë‹ˆí‹° ì†Œì‹",
        "tasks": [
            {"type": "news", "params": {"categories": ["community", "ctf"]}},
        ],
    },
    "thu": {
        "title": "ğŸ’Š ëª©ìš”ì¼: NF ì¹˜ë£Œì œ ê°œë°œ / ì„ìƒì‹œí—˜",
        "tasks": [
            {"type": "clinical_trials", "params": {"max_results": 15}},
            {"type": "news", "params": {"categories": ["treatment", "ctf"]}},
        ],
    },
    "fri": {
        "title": "ğŸ“‹ ê¸ˆìš”ì¼: í¬ê·€ì§ˆí™˜ ì •ì±…/ì œë„ ë‰´ìŠ¤",
        "tasks": [
            {"type": "news", "params": {"categories": ["policy_kr", "policy_global", "rarenote"]}},
        ],
    },
    "sat": {
        "title": "ğŸŒ¿ í† ìš”ì¼: NF í™˜ì ì¼ìƒ / íë§ ì½˜í…ì¸ ",
        "tasks": [
            {"type": "patient_stories", "params": {"sources": ["healing"]}},
            {"type": "news", "params": {"categories": ["rarenote"]}},
        ],
    },
    "sun": {
        "title": "ğŸ“° ì¼ìš”ì¼: ì£¼ê°„ í•˜ì´ë¼ì´íŠ¸ + ë‹¤ìŒ ì£¼ ì˜ˆê³ ",
        "tasks": [
            {"type": "weekly_summary", "params": {}},
        ],
    },
}


class DailyOrchestrator:
    """ì¼ì¼ ì½˜í…ì¸  ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self):
        self.pubmed = PubMedFetcher()
        self.news = NewsFetcher()
        self.trials = ClinicalTrialsFetcher()
        self.stories = PatientStoryFetcher()
        self.results = {}
        self.errors = []
        self.seen_hashes = self._load_seen_items()

    # â”€â”€ ì¤‘ë³µ ì œê±° â”€â”€

    def _load_seen_items(self) -> set:
        """ì´ì „ì— ìˆ˜ì§‘í•œ ì•„ì´í…œ í•´ì‹œ ë¡œë“œ"""
        if os.path.exists(DEDUP_FILE):
            try:
                with open(DEDUP_FILE, "r") as f:
                    data = json.load(f)
                    # ìµœê·¼ 30ì¼ í•­ëª©ë§Œ ìœ ì§€
                    cutoff = (datetime.now() - timedelta(days=30)).isoformat()
                    recent = {k: v for k, v in data.items() if v > cutoff}
                    return set(recent.keys())
            except Exception:
                pass
        return set()

    def _save_seen_items(self):
        """ìˆ˜ì§‘í•œ ì•„ì´í…œ í•´ì‹œ ì €ì¥"""
        data = {h: datetime.now().isoformat() for h in self.seen_hashes}
        with open(DEDUP_FILE, "w") as f:
            json.dump(data, f)

    def _item_hash(self, item: dict) -> str:
        """ì•„ì´í…œ ê³ ìœ  í•´ì‹œ ìƒì„±"""
        key_parts = []
        for k in ["pmid", "nct_id", "url", "link", "title"]:
            if k in item and item[k]:
                key_parts.append(str(item[k]))
        key = "|".join(key_parts)
        return hashlib.md5(key.encode()).hexdigest()

    def deduplicate(self, items: list) -> list:
        """ì¤‘ë³µ ì•„ì´í…œ ì œê±°"""
        unique = []
        for item in items:
            h = self._item_hash(item)
            if h not in self.seen_hashes:
                self.seen_hashes.add(h)
                unique.append(item)
        return unique

    # â”€â”€ ë°ì´í„° ì •ê·œí™” â”€â”€

    def normalize_item(self, item: dict, source_type: str) -> dict:
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
        normalized = {
            "id": self._item_hash(item),
            "source_type": source_type,
            "title": "",
            "summary": "",
            "url": "",
            "language": "en",
            "relevance_score": 0,
            "collected_at": datetime.now().isoformat(),
            "raw_data": item,
        }

        if source_type == "pubmed":
            normalized.update({
                "title": item.get("title", ""),
                "summary": item.get("abstract", "")[:300],
                "url": item.get("url", ""),
                "language": "en",
                "authors": item.get("authors", []),
                "journal": item.get("journal", ""),
                "pub_date": item.get("pub_date", ""),
            })

        elif source_type == "clinical_trial":
            normalized.update({
                "title": item.get("title", ""),
                "summary": item.get("brief_summary", "")[:300],
                "url": item.get("url", ""),
                "language": "en",
                "status": item.get("status", ""),
                "phase": item.get("phase", []),
                "sponsor": item.get("sponsor", ""),
            })

        elif source_type == "news":
            normalized.update({
                "title": item.get("title", ""),
                "summary": item.get("description", "")[:300],
                "url": item.get("link", "") or item.get("url", ""),
                "language": item.get("language", "en"),
                "source_name": item.get("source_name", ""),
                "pub_date": item.get("pub_date", ""),
            })

        elif source_type == "patient_story":
            normalized.update({
                "title": item.get("title", ""),
                "summary": item.get("body", "")[:300],
                "url": item.get("url", ""),
                "language": "en",
                "positivity_score": item.get("positivity_score", 0),
            })

        # NF ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        normalized["relevance_score"] = self._calc_relevance(normalized)

        return normalized

    def _calc_relevance(self, item: dict) -> int:
        """NF ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()

        high_keywords = ["neurofibromatosis", "nf1", "nf2", "schwannomatosis",
                         "plexiform", "selumetinib", "koselugo", "ì‹ ê²½ì„¬ìœ ì¢…"]
        medium_keywords = ["neurofibroma", "mek inhibitor", "rare disease",
                           "í¬ê·€ì§ˆí™˜", "ìœ ì „ì§ˆí™˜", "ctf", "children's tumor"]

        for kw in high_keywords:
            if kw in text:
                score += 3
        for kw in medium_keywords:
            if kw in text:
                score += 1

        return min(score, 10)

    # â”€â”€ íƒœìŠ¤í¬ ì‹¤í–‰ê¸° â”€â”€

    def run_pubmed(self, params: dict) -> list:
        """PubMed ë…¼ë¬¸ ìˆ˜ì§‘ íƒœìŠ¤í¬"""
        logger.info("ğŸ“š PubMed ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘")
        try:
            results = self.pubmed.fetch_nf_latest(
                days_back=params.get("days_back", 7),
                max_per_query=params.get("max_per_query", 5),
            )
            all_articles = []
            for query, articles in results.items():
                for art in articles:
                    normalized = self.normalize_item(art, "pubmed")
                    all_articles.append(normalized)
            return self.deduplicate(all_articles)
        except Exception as e:
            logger.error(f"âŒ PubMed ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.errors.append({"task": "pubmed", "error": str(e)})
            return []

    def run_news(self, params: dict) -> list:
        """ë‰´ìŠ¤/RSS ìˆ˜ì§‘ íƒœìŠ¤í¬"""
        categories = params.get("categories", [])
        logger.info(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {categories}")
        all_news = []
        try:
            for cat in categories:
                items = self.news.fetch_category(cat, max_per_feed=10)
                for item in items:
                    normalized = self.normalize_item(item, "news")
                    all_news.append(normalized)
            return self.deduplicate(all_news)
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.errors.append({"task": "news", "error": str(e)})
            return []

    def run_clinical_trials(self, params: dict) -> list:
        """ì„ìƒì‹œí—˜ ìˆ˜ì§‘ íƒœìŠ¤í¬"""
        logger.info("ğŸ”¬ ì„ìƒì‹œí—˜ ìˆ˜ì§‘ ì‹œì‘")
        try:
            results = self.trials.fetch_all_nf(
                max_per_query=params.get("max_results", 10),
            )
            all_trials = []
            for query, trials in results.items():
                for trial in trials:
                    normalized = self.normalize_item(trial, "clinical_trial")
                    all_trials.append(normalized)
            return self.deduplicate(all_trials)
        except Exception as e:
            logger.error(f"âŒ ì„ìƒì‹œí—˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.errors.append({"task": "clinical_trials", "error": str(e)})
            return []

    def run_patient_stories(self, params: dict) -> list:
        """í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ íƒœìŠ¤í¬"""
        sources = params.get("sources", ["all"])
        logger.info(f"ğŸ’› í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ ì‹œì‘: {sources}")
        all_stories = []
        try:
            if "reddit" in sources or "all" in sources:
                reddit = self.stories.fetch_reddit(max_results=15)
                for item in reddit:
                    all_stories.append(self.normalize_item(item, "patient_story"))

            if "ctf" in sources or "all" in sources:
                ctf = self.stories.fetch_ctf_stories()
                for item in ctf:
                    all_stories.append(self.normalize_item(item, "patient_story"))

            if "healing" in sources:
                healing = self.stories.fetch_healing_content()
                for item in healing:
                    all_stories.append(self.normalize_item(item, "patient_story"))

            return self.deduplicate(all_stories)
        except Exception as e:
            logger.error(f"âŒ í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.errors.append({"task": "patient_stories", "error": str(e)})
            return []

    def run_weekly_summary(self, params: dict) -> list:
        """ì£¼ê°„ í•˜ì´ë¼ì´íŠ¸ ìƒì„± (ì´ë²ˆ ì£¼ ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜)"""
        logger.info("ğŸ“° ì£¼ê°„ í•˜ì´ë¼ì´íŠ¸ ìƒì„±")

        # ì´ë²ˆ ì£¼ ì•„ì¹´ì´ë¸Œ íŒŒì¼ ë¡œë“œ
        week_data = []
        today = datetime.now()
        for i in range(6):  # ì›”~í† 
            day = today - timedelta(days=today.weekday() - i)
            filename = f"daily_{day.strftime('%Y%m%d')}.json"
            filepath = os.path.join(DATA_DIR, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        week_data.extend(data.get("items", []))
                except Exception:
                    continue

        # ê´€ë ¨ì„± ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        week_data.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

        summary = {
            "type": "weekly_summary",
            "title": f"ì£¼ê°„ í•˜ì´ë¼ì´íŠ¸ ({today.strftime('%Y.%m.%d')})",
            "total_items_this_week": len(week_data),
            "top_items": week_data[:10],  # ìƒìœ„ 10ê°œ
            "generated_at": datetime.now().isoformat(),
        }

        return [summary]

    # â”€â”€ ë©”ì¸ ì‹¤í–‰ â”€â”€

    def run_day(self, day: str, dry_run: bool = False) -> dict:
        """
        íŠ¹ì • ìš”ì¼ì˜ ì „ì²´ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            day: mon, tue, wed, thu, fri, sat, sun
            dry_run: Trueë©´ ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥

        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        plan = DAY_PLANS.get(day)
        if not plan:
            logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì¼: {day}")
            return {}

        logger.info("=" * 70)
        logger.info(f"ğŸš€ {plan['title']}")
        logger.info(f"   ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M KST')}")
        logger.info(f"   íƒœìŠ¤í¬: {len(plan['tasks'])}ê°œ")
        logger.info("=" * 70)

        if dry_run:
            logger.info("ğŸƒ DRY RUN - ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.")
            for i, task in enumerate(plan["tasks"], 1):
                logger.info(f"  {i}. [{task['type']}] params={task['params']}")
            return {"plan": plan, "dry_run": True}

        # íƒœìŠ¤í¬ ì‹¤í–‰
        all_items = []
        task_results = {}

        task_runner = {
            "pubmed": self.run_pubmed,
            "news": self.run_news,
            "clinical_trials": self.run_clinical_trials,
            "patient_stories": self.run_patient_stories,
            "weekly_summary": self.run_weekly_summary,
        }

        for task in plan["tasks"]:
            task_type = task["type"]
            task_params = task["params"]

            logger.info(f"\n{'â”€'*50}")
            logger.info(f"â–¶ íƒœìŠ¤í¬: {task_type}")

            runner = task_runner.get(task_type)
            if runner:
                try:
                    items = runner(task_params)
                    task_results[task_type] = len(items)
                    all_items.extend(items)
                    logger.info(f"  âœ… {len(items)}ê±´ ìˆ˜ì§‘")
                except Exception as e:
                    logger.error(f"  âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    logger.error(traceback.format_exc())
                    self.errors.append({"task": task_type, "error": str(e)})
            else:
                logger.warning(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬ íƒ€ì…: {task_type}")

        # ê´€ë ¨ì„± ì ìˆ˜ ìˆœ ì •ë ¬
        all_items.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

        # ê²°ê³¼ ì €ì¥
        result = {
            "day": day,
            "title": plan["title"],
            "date": datetime.now().strftime("%Y-%m-%d"),
            "total_items": len(all_items),
            "task_results": task_results,
            "errors": self.errors,
            "items": all_items,
            "generated_at": datetime.now().isoformat(),
        }

        self._save_daily_result(day, result)
        self._save_seen_items()

        # ì‹¤í–‰ ìš”ì•½
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½")
        logger.info(f"   ì´ ìˆ˜ì§‘: {len(all_items)}ê±´")
        for task_type, count in task_results.items():
            logger.info(f"   - {task_type}: {count}ê±´")
        if self.errors:
            logger.warning(f"   âš ï¸ ì—ëŸ¬: {len(self.errors)}ê±´")
            for err in self.errors:
                logger.warning(f"     - {err['task']}: {err['error']}")
        logger.info(f"{'='*70}")

        return result

    def _save_daily_result(self, day: str, result: dict):
        """ì¼ì¼ ê²°ê³¼ ì €ì¥"""
        date_str = datetime.now().strftime("%Y%m%d")

        # ë‹¹ì¼ ê²°ê³¼
        filename = f"daily_{date_str}.json"
        filepath = os.path.join(DATA_DIR, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ì €ì¥: {filepath}")

        # ì•„ì¹´ì´ë¸Œ
        archive_filename = f"daily_{day}_{date_str}.json"
        archive_path = os.path.join(ARCHIVE_DIR, archive_filename)
        with open(archive_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“¦ ì•„ì¹´ì´ë¸Œ: {archive_path}")

    def run_all_days(self, dry_run: bool = False):
        """ëª¨ë“  ìš”ì¼ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)"""
        for day in ["mon", "tue", "wed", "thu", "fri", "sat", "sun"]:
            self.errors = []
            self.run_day(day, dry_run)


def get_today_day() -> str:
    """KST ê¸°ì¤€ ì˜¤ëŠ˜ ìš”ì¼ ë°˜í™˜"""
    # UTC+9 ì ìš©
    from datetime import timezone
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    day_map = {0: "mon", 1: "tue", 2: "wed", 3: "thu", 4: "fri", 5: "sat", 6: "sun"}
    return day_map[now.weekday()]


def main():
    parser = argparse.ArgumentParser(description="END NF ì¼ì¼ ì½˜í…ì¸  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°")
    parser.add_argument("--day", type=str, default="",
                        help="ì‹¤í–‰ ìš”ì¼ (mon~sun ë˜ëŠ” all). ë¯¸ì§€ì • ì‹œ ì˜¤ëŠ˜ ìš”ì¼")
    parser.add_argument("--dry-run", action="store_true",
                        help="ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ ì¶œë ¥")
    args = parser.parse_args()

    orchestrator = DailyOrchestrator()

    if args.day == "all":
        orchestrator.run_all_days(args.dry_run)
    else:
        day = args.day or get_today_day()
        logger.info(f"ğŸ“… ì‹¤í–‰ ìš”ì¼: {day.upper()}")
        orchestrator.run_day(day, args.dry_run)


if __name__ == "__main__":
    main()

"""
============================================================
END NF ì½˜í…ì¸  ì‹œìŠ¤í…œ - ë‰´ìŠ¤/RSS ìˆ˜ì§‘ê¸°
============================================================
Google News RSS, NORD, CTF ë“±ì—ì„œ NF ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python news_fetcher.py
    python news_fetcher.py --category treatment
    python news_fetcher.py --category policy_kr
"""

import os
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from html import unescape
import re
import argparse

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    import urllib.request
    HAS_REQUESTS = False


def http_get(url: str, timeout: int = 30) -> str:
    if HAS_REQUESTS:
        resp = requests.get(url, timeout=timeout, headers={"User-Agent": "ENDNF-ContentBot/1.0"})
        resp.raise_for_status()
        return resp.text
    else:
        req = urllib.request.Request(url, headers={"User-Agent": "ENDNF-ContentBot/1.0"})
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            return resp.read().decode("utf-8")


# â”€â”€ RSS í”¼ë“œ ì†ŒìŠ¤ ì •ì˜ â”€â”€
RSS_FEEDS = {
    # â­ í•µì‹¬ ì°¸ê³ ì²˜: ë ˆì–´ë…¸íŠ¸ + CTF
    "rarenote": [
        {
            "name": "ë ˆì–´ë…¸íŠ¸ ë‰´ìŠ¤",
            "url": "https://rarenote.io/news",
            "lang": "ko",
            "type": "scrape",
        },
    ],

    "ctf": [
        {
            "name": "CTF News",
            "url": "https://www.ctf.org/news/",
            "lang": "en",
            "type": "scrape",
        },
        {
            "name": "CTF Drug Pipeline",
            "url": "https://www.ctf.org/clinical-drug-pipeline/",
            "lang": "en",
            "type": "scrape",
        },
    ],

    # ìˆ˜ìš”ì¼: í•´ì™¸ ì»¤ë®¤ë‹ˆí‹°
    "community": [
        {
            "name": "Google News - NF Community",
            "url": "https://news.google.com/rss/search?q=neurofibromatosis+community+foundation&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
        {
            "name": "Google News - NF Awareness",
            "url": "https://news.google.com/rss/search?q=neurofibromatosis+awareness+event&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
    ],

    # ëª©ìš”ì¼: ì¹˜ë£Œì œ/ì„ìƒ
    "treatment": [
        {
            "name": "Google News - NF Treatment",
            "url": "https://news.google.com/rss/search?q=neurofibromatosis+treatment+drug+2025+OR+2026&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
        {
            "name": "Google News - Selumetinib/Koselugo",
            "url": "https://news.google.com/rss/search?q=selumetinib+OR+koselugo+neurofibromatosis&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
        {
            "name": "Google News - NF Clinical Trial",
            "url": "https://news.google.com/rss/search?q=neurofibromatosis+clinical+trial&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
    ],

    # ê¸ˆìš”ì¼: ì •ì±…/ì œë„ (í•œêµ­)
    "policy_kr": [
        {
            "name": "Google News - í¬ê·€ì§ˆí™˜ ì •ì±…",
            "url": "https://news.google.com/rss/search?q=%ED%9D%AC%EA%B7%80%EC%A7%88%ED%99%98+%EC%A0%95%EC%B1%85&hl=ko&gl=KR&ceid=KR:ko",
            "lang": "ko",
        },
        {
            "name": "Google News - ì‹ ê²½ì„¬ìœ ì¢…",
            "url": "https://news.google.com/rss/search?q=%EC%8B%A0%EA%B2%BD%EC%84%AC%EC%9C%A0%EC%A2%85&hl=ko&gl=KR&ceid=KR:ko",
            "lang": "ko",
        },
        {
            "name": "Google News - í¬ê·€ì§ˆí™˜ ê±´ê°•ë³´í—˜",
            "url": "https://news.google.com/rss/search?q=%ED%9D%AC%EA%B7%80%EC%A7%88%ED%99%98+%EA%B1%B4%EA%B0%95%EB%B3%B4%ED%97%98&hl=ko&gl=KR&ceid=KR:ko",
            "lang": "ko",
        },
    ],

    # ê¸ˆìš”ì¼: ì •ì±…/ì œë„ (ê¸€ë¡œë²Œ)
    "policy_global": [
        {
            "name": "Google News - Rare Disease Policy",
            "url": "https://news.google.com/rss/search?q=rare+disease+policy+regulation+2025+OR+2026&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
        {
            "name": "Google News - Orphan Drug",
            "url": "https://news.google.com/rss/search?q=orphan+drug+neurofibromatosis&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
    ],

    # ì¼ë°˜ NF ë‰´ìŠ¤
    "general": [
        {
            "name": "Google News - Neurofibromatosis",
            "url": "https://news.google.com/rss/search?q=neurofibromatosis&hl=en&gl=US&ceid=US:en",
            "lang": "en",
        },
    ],
}


class NewsFetcher:
    """ë‰´ìŠ¤/RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.collected = []

    def scrape_page(self, url: str, source_name: str, lang: str = "ko", max_items: int = 10) -> list:
        """
        ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (ë ˆì–´ë…¸íŠ¸, CTF ë“±)

        Args:
            url: í˜ì´ì§€ URL
            source_name: ì†ŒìŠ¤ ì´ë¦„
            lang: ì–¸ì–´
            max_items: ìµœëŒ€ ì•„ì´í…œ ìˆ˜

        Returns:
            ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"  ğŸŒ ìŠ¤í¬ë˜í•‘: {source_name}")

        try:
            html = http_get(url)
        except Exception as e:
            print(f"     âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []

        items = []

        # ë§í¬ + ì œëª© íŒ¨í„´ ì¶”ì¶œ
        # <a href="..." ...>ì œëª©</a> ë˜ëŠ” <h2/h3>ì œëª©</h2/h3>
        link_patterns = re.findall(
            r'<a[^>]+href="([^"]*)"[^>]*>([^<]{5,100})</a>',
            html
        )

        # ë ˆì–´ë…¸íŠ¸ íŠ¹í™”: /contents/, /news/ ê²½ë¡œ
        if "rarenote" in url:
            for href, title in link_patterns:
                if any(path in href for path in ["/contents/", "/news/"]):
                    full_url = href if href.startswith("http") else f"https://rarenote.io{href}"
                    clean_title = self._clean_html(title)
                    if len(clean_title) > 5:
                        items.append({
                            "title": clean_title,
                            "link": full_url,
                            "description": "",
                            "pub_date": "",
                            "source_name": source_name,
                            "language": lang,
                            "category": "rarenote",
                            "fetched_at": datetime.now().isoformat(),
                        })

        # CTF íŠ¹í™”: /news/, /storiesofnf/, /clinical-drug-pipeline/
        elif "ctf.org" in url:
            for href, title in link_patterns:
                if any(path in href for path in ["/news/", "/storiesofnf/", "/clinical-drug-pipeline/"]):
                    full_url = href if href.startswith("http") else f"https://www.ctf.org{href}"
                    clean_title = self._clean_html(title)
                    if len(clean_title) > 5:
                        items.append({
                            "title": clean_title,
                            "link": full_url,
                            "description": "",
                            "pub_date": "",
                            "source_name": source_name,
                            "language": lang,
                            "category": "ctf",
                            "fetched_at": datetime.now().isoformat(),
                        })

        # ì¼ë°˜ ìŠ¤í¬ë˜í•‘
        else:
            for href, title in link_patterns[:max_items]:
                full_url = href if href.startswith("http") else f"{url.rstrip('/')}/{href.lstrip('/')}"
                clean_title = self._clean_html(title)
                if len(clean_title) > 5:
                    items.append({
                        "title": clean_title,
                        "link": full_url,
                        "description": "",
                        "pub_date": "",
                        "source_name": source_name,
                        "language": lang,
                        "fetched_at": datetime.now().isoformat(),
                    })

        items = items[:max_items]
        print(f"     â†’ {len(items)}ê±´ ìˆ˜ì§‘")
        return items

    def fetch_rss(self, url: str, source_name: str, lang: str = "en", max_items: int = 10) -> list:
        """
        RSS í”¼ë“œ íŒŒì‹±

        Args:
            url: RSS í”¼ë“œ URL
            source_name: ì†ŒìŠ¤ ì´ë¦„
            lang: ì–¸ì–´ ì½”ë“œ
            max_items: ìµœëŒ€ ì•„ì´í…œ ìˆ˜

        Returns:
            ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"  ğŸ“° ìˆ˜ì§‘: {source_name}")

        try:
            xml_text = http_get(url)
        except Exception as e:
            print(f"     âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

        items = []
        try:
            root = ET.fromstring(xml_text)

            # RSS 2.0 í˜•ì‹
            for item in root.findall(".//item")[:max_items]:
                title = item.findtext("title", "")
                link = item.findtext("link", "")
                description = item.findtext("description", "")
                pub_date = item.findtext("pubDate", "")
                source_tag = item.findtext("source", "")

                # HTML íƒœê·¸ ì œê±° + unescape
                title = self._clean_html(title)
                description = self._clean_html(description)

                items.append({
                    "title": title,
                    "link": link,
                    "description": description[:500],
                    "pub_date": pub_date,
                    "source_name": source_name,
                    "original_source": source_tag,
                    "language": lang,
                    "fetched_at": datetime.now().isoformat(),
                })

            # Atom í˜•ì‹ (ëŒ€ì²´)
            if not items:
                ns = {"atom": "http://www.w3.org/2005/Atom"}
                for entry in root.findall(".//atom:entry", ns)[:max_items]:
                    title = entry.findtext("atom:title", "", ns)
                    link_elem = entry.find("atom:link", ns)
                    link = link_elem.get("href", "") if link_elem is not None else ""
                    summary = entry.findtext("atom:summary", "", ns)
                    updated = entry.findtext("atom:updated", "", ns)

                    items.append({
                        "title": self._clean_html(title),
                        "link": link,
                        "description": self._clean_html(summary)[:500],
                        "pub_date": updated,
                        "source_name": source_name,
                        "language": lang,
                        "fetched_at": datetime.now().isoformat(),
                    })

        except ET.ParseError as e:
            print(f"     âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

        print(f"     â†’ {len(items)}ê±´ ìˆ˜ì§‘")
        return items

    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        text = unescape(text)
        text = re.sub(r"<[^>]+>", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    def fetch_category(self, category: str, max_per_feed: int = 10) -> list:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  RSS í”¼ë“œ ìˆ˜ì§‘

        Args:
            category: RSS_FEEDS í‚¤ (community, treatment, policy_kr ë“±)
            max_per_feed: í”¼ë“œë‹¹ ìµœëŒ€ ì•„ì´í…œ ìˆ˜

        Returns:
            ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        """
        feeds = RSS_FEEDS.get(category, [])
        if not feeds:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(RSS_FEEDS.keys())}")
            return []

        print(f"\n{'='*60}")
        print(f"ğŸ“° [{category.upper()}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        print(f"   í”¼ë“œ ìˆ˜: {len(feeds)}")
        print(f"{'='*60}")

        all_items = []
        seen_links = set()

        for feed in feeds:
            feed_type = feed.get("type", "rss")

            if feed_type == "scrape":
                items = self.scrape_page(
                    url=feed["url"],
                    source_name=feed["name"],
                    lang=feed.get("lang", "en"),
                    max_items=max_per_feed,
                )
            else:
                items = self.fetch_rss(
                    url=feed["url"],
                    source_name=feed["name"],
                    lang=feed.get("lang", "en"),
                    max_items=max_per_feed,
                )

            # ì¤‘ë³µ ë§í¬ ì œê±°
            for item in items:
                if item["link"] not in seen_links:
                    seen_links.add(item["link"])
                    item["category"] = category
                    all_items.append(item)

        print(f"\nâœ… [{category}] ì´ {len(all_items)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±°)")
        return all_items

    def fetch_all(self, max_per_feed: int = 10) -> dict:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘"""
        results = {}
        for category in RSS_FEEDS:
            results[category] = self.fetch_category(category, max_per_feed)
        return results

    def fetch_by_day(self, day_of_week: str, max_per_feed: int = 10) -> list:
        """
        ìš”ì¼ì— ë§ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            day_of_week: mon, tue, wed, thu, fri, sat, sun

        Returns:
            ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        """
        day_category_map = {
            "mon": ["general", "rarenote", "ctf"],    # ì›”: NF ë‰´ìŠ¤ + í•µì‹¬ ì°¸ê³ ì²˜
            "tue": ["rarenote"],                       # í™”: í™˜ì ì´ì•¼ê¸° (+ ë³„ë„ ìˆ˜ì§‘ê¸°)
            "wed": ["community", "ctf"],               # ìˆ˜: í•´ì™¸ ì»¤ë®¤ë‹ˆí‹° + CTF
            "thu": ["treatment", "ctf"],               # ëª©: ì¹˜ë£Œì œ/ì„ìƒ + CTF Pipeline
            "fri": ["policy_kr", "policy_global", "rarenote"],  # ê¸ˆ: ì •ì±… + ë ˆì–´ë…¸íŠ¸
            "sat": ["rarenote"],                        # í† : íë§ (+ ë³„ë„ ìˆ˜ì§‘ê¸°)
            "sun": [],                                  # ì¼: ì£¼ê°„ í•˜ì´ë¼ì´íŠ¸ (ìë™ìƒì„±)
        }

        categories = day_category_map.get(day_of_week.lower(), [])
        if not categories:
            print(f"â„¹ï¸ {day_of_week}ì—ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        all_items = []
        for cat in categories:
            all_items.extend(self.fetch_category(cat, max_per_feed))

        return all_items


def save_results(data, filename: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = os.path.join(os.path.dirname(__file__), "data")
    os.makedirs(output_dir, exist_ok=True)

    filepath = os.path.join(output_dir, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath


def main():
    parser = argparse.ArgumentParser(description="END NF ë‰´ìŠ¤/RSS ìˆ˜ì§‘ê¸°")
    parser.add_argument("--category", type=str, default="",
                        help="ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬ (community, treatment, policy_kr, policy_global, general)")
    parser.add_argument("--day", type=str, default="",
                        help="ìš”ì¼ë³„ ìˆ˜ì§‘ (mon, tue, wed, thu, fri, sat, sun)")
    parser.add_argument("--all", action="store_true", help="ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘")
    parser.add_argument("--max", type=int, default=10, help="í”¼ë“œë‹¹ ìµœëŒ€ ì•„ì´í…œ ìˆ˜")
    parser.add_argument("--output", type=str, default="", help="ì¶œë ¥ íŒŒì¼ëª…")
    args = parser.parse_args()

    fetcher = NewsFetcher()

    if args.all:
        results = fetcher.fetch_all(args.max)
        filename = args.output or f"news_all_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)

    elif args.day:
        results = fetcher.fetch_by_day(args.day, args.max)
        filename = args.output or f"news_{args.day}_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)

    elif args.category:
        results = fetcher.fetch_category(args.category, args.max)
        filename = args.output or f"news_{args.category}_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)

    else:
        # ì˜¤ëŠ˜ ìš”ì¼ì— ë§ê²Œ ìë™ ìˆ˜ì§‘
        day_map = {0: "mon", 1: "tue", 2: "wed", 3: "thu", 4: "fri", 5: "sat", 6: "sun"}
        today = day_map[datetime.now().weekday()]
        print(f"ğŸ“… ì˜¤ëŠ˜ì€ {today.upper()}ìš”ì¼ì…ë‹ˆë‹¤. í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        results = fetcher.fetch_by_day(today, args.max)
        filename = args.output or f"news_{today}_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)


if __name__ == "__main__":
    main()

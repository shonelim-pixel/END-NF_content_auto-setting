"""
============================================================
END NF ì½˜í…ì¸  ì‹œìŠ¤í…œ - í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ê¸°
============================================================
CTF Stories of NF, Reddit, NF Network ë“±ì—ì„œ ê¸ì •ì ì¸ í™˜ì ì´ì•¼ê¸°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python patient_story_fetcher.py
    python patient_story_fetcher.py --source reddit
    python patient_story_fetcher.py --source ctf
"""

import os
import json
import re
from datetime import datetime
from html import unescape
import argparse

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    import urllib.request
    HAS_REQUESTS = False


def http_get(url: str, timeout: int = 30, headers: dict = None) -> str:
    default_headers = {"User-Agent": "ENDNF-ContentBot/1.0 (educational; NF patient support)"}
    if headers:
        default_headers.update(headers)

    if HAS_REQUESTS:
        resp = requests.get(url, timeout=timeout, headers=default_headers)
        resp.raise_for_status()
        return resp.text
    else:
        req = urllib.request.Request(url, headers=default_headers)
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            return resp.read().decode("utf-8")


# â”€â”€ ê¸ì •ì  ìŠ¤í† ë¦¬ í•„í„°ë§ í‚¤ì›Œë“œ â”€â”€
POSITIVE_KEYWORDS = [
    # ì˜ì–´
    "hope", "hopeful", "grateful", "thankful", "blessed",
    "milestone", "achievement", "success", "progress",
    "positive", "happy", "joy", "love", "strength", "strong",
    "overcome", "survivor", "warrior", "brave", "courage",
    "support", "community", "together", "inspire", "inspiring",
    "celebrate", "win", "victory", "improve", "better",
    "healing", "recovery", "treatment worked", "good news",
    # í•œêµ­ì–´
    "í¬ë§", "ê°ì‚¬", "ê·¹ë³µ", "ì‘ì›", "í•¨ê»˜", "ì‚¬ë‘", "í–‰ë³µ",
    "ì¹˜ë£Œ", "íšŒë³µ", "í˜", "ìš©ê¸°", "ê¸ì •", "ì¢‹ì€ ì†Œì‹",
]

NEGATIVE_KEYWORDS = [
    "suicide", "kill myself", "give up", "hopeless", "worthless",
    "ìì‚´", "í¬ê¸°", "ì ˆë§",
]


class PatientStoryFetcher:
    """NF í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ê¸°"""

    def fetch_reddit(self, max_results: int = 20, time_filter: str = "month") -> list:
        """
        Reddit r/neurofibromatosisì—ì„œ ê¸ì •ì  ê²Œì‹œë¬¼ ìˆ˜ì§‘

        Args:
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            time_filter: week, month, year, all

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        print("=" * 60)
        print("ğŸ”µ Reddit r/neurofibromatosis ìˆ˜ì§‘")
        print("=" * 60)

        stories = []

        # Reddit JSON API (ì¸ì¦ ì—†ì´ ì ‘ê·¼ ê°€ëŠ¥)
        urls = [
            f"https://www.reddit.com/r/neurofibromatosis/top.json?t={time_filter}&limit={max_results}",
            f"https://www.reddit.com/r/neurofibromatosis/hot.json?limit={max_results}",
        ]

        for url in urls:
            print(f"  ğŸ“¥ ìˆ˜ì§‘ ì¤‘: {url[:60]}...")
            try:
                data = json.loads(http_get(url))
                posts = data.get("data", {}).get("children", [])

                for post in posts:
                    pd = post.get("data", {})
                    title = pd.get("title", "")
                    body = pd.get("selftext", "")
                    score = pd.get("score", 0)
                    created = pd.get("created_utc", 0)
                    permalink = pd.get("permalink", "")
                    num_comments = pd.get("num_comments", 0)

                    # ë¶€ì •ì  ì½˜í…ì¸  í•„í„°ë§
                    full_text = f"{title} {body}".lower()
                    if any(neg in full_text for neg in NEGATIVE_KEYWORDS):
                        continue

                    # ê¸ì •ë„ ì ìˆ˜ ê³„ì‚°
                    positivity = sum(1 for kw in POSITIVE_KEYWORDS if kw.lower() in full_text)

                    stories.append({
                        "source": "reddit",
                        "title": title,
                        "body": body[:1000],
                        "score": score,
                        "positivity_score": positivity,
                        "num_comments": num_comments,
                        "url": f"https://reddit.com{permalink}",
                        "created_at": datetime.fromtimestamp(created).isoformat() if created else "",
                        "fetched_at": datetime.now().isoformat(),
                    })

                print(f"     â†’ {len(posts)}ê±´ ìˆ˜ì§‘")

            except Exception as e:
                print(f"     âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # ì¤‘ë³µ ì œê±° + ê¸ì •ë„ ìˆœ ì •ë ¬
        seen_urls = set()
        unique = []
        for story in stories:
            if story["url"] not in seen_urls:
                seen_urls.add(story["url"])
                unique.append(story)

        unique.sort(key=lambda x: (x["positivity_score"], x["score"]), reverse=True)
        print(f"\nâœ… Reddit: ì´ {len(unique)}ê±´ ìˆ˜ì§‘ (ê¸ì •ë„ ìˆœ ì •ë ¬)")
        return unique

    def fetch_ctf_stories(self) -> list:
        """
        CTF Stories of NF í˜ì´ì§€ì—ì„œ í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘
        (ì›¹ ìŠ¤í¬ë˜í•‘ - êµ¬ì¡°ê°€ ë³€ê²½ë  ìˆ˜ ìˆìŒ)
        """
        print("\n" + "=" * 60)
        print("ğŸ’™ CTF Stories of NF ìˆ˜ì§‘")
        print("=" * 60)

        stories = []
        url = "https://www.ctf.org/storiesofnf/"

        try:
            html = http_get(url)

            # ê°„ë‹¨í•œ HTML íŒŒì‹± (BeautifulSoup ì—†ì´)
            # CTF ì‚¬ì´íŠ¸ì˜ ìŠ¤í† ë¦¬ ì¹´ë“œ íŒ¨í„´ ë§¤ì¹­
            # ì‹¤ì œ ìš´ì˜ ì‹œ BeautifulSoup/Scrapyë¡œ êµì²´ ê¶Œì¥

            # <a href="/storiesofnf/..." íŒ¨í„´ ì¶”ì¶œ
            story_links = re.findall(
                r'href="(/storiesofnf/[^"]+)"[^>]*>',
                html
            )

            # ì œëª© íŒ¨í„´
            titles = re.findall(
                r'<h[23][^>]*class="[^"]*"[^>]*>([^<]+)</h[23]>',
                html
            )

            print(f"  â†’ ìŠ¤í† ë¦¬ ë§í¬ {len(story_links)}ê°œ ë°œê²¬")
            print(f"  â†’ ì œëª© {len(titles)}ê°œ ë°œê²¬")

            # ê³ ìœ  ë§í¬ë§Œ ìˆ˜ì§‘
            seen = set()
            for link in story_links:
                if link not in seen:
                    seen.add(link)
                    stories.append({
                        "source": "ctf_stories",
                        "title": "",
                        "url": f"https://www.ctf.org{link}",
                        "fetched_at": datetime.now().isoformat(),
                        "note": "ìƒì„¸ ë‚´ìš©ì€ ê°œë³„ í˜ì´ì§€ ë°©ë¬¸ í•„ìš”",
                    })

            print(f"\nâœ… CTF Stories: {len(stories)}ê±´ ë§í¬ ìˆ˜ì§‘")

        except Exception as e:
            print(f"  âŒ CTF ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return stories

    def fetch_ctf_news_stories(self) -> list:
        """CTF ë‰´ìŠ¤ì—ì„œ í™˜ì ê´€ë ¨ ìŠ¤í† ë¦¬ ìˆ˜ì§‘"""
        print("\n" + "=" * 60)
        print("ğŸ“° CTF News - í™˜ì ìŠ¤í† ë¦¬ í•„í„°ë§")
        print("=" * 60)

        stories = []
        url = "https://www.ctf.org/news/"

        try:
            html = http_get(url)

            # ë‰´ìŠ¤ ì•„ì´í…œ ë§í¬ ì¶”ì¶œ
            news_links = re.findall(
                r'href="(/news/[^"]+)"',
                html
            )

            seen = set()
            for link in news_links:
                if link not in seen and link != "/news/":
                    seen.add(link)
                    stories.append({
                        "source": "ctf_news",
                        "url": f"https://www.ctf.org{link}",
                        "fetched_at": datetime.now().isoformat(),
                    })

            print(f"  â†’ {len(stories)}ê°œ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘")

        except Exception as e:
            print(f"  âŒ CTF ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return stories

    def fetch_healing_content(self) -> list:
        """í† ìš”ì¼ìš© íë§ ì½˜í…ì¸  ì†ŒìŠ¤ ìˆ˜ì§‘"""
        print("\n" + "=" * 60)
        print("ğŸŒ¿ íë§ ì½˜í…ì¸  ìˆ˜ì§‘")
        print("=" * 60)

        items = []

        # Redditì—ì„œ íë§/ì¼ìƒ ê´€ë ¨ ê²Œì‹œë¬¼
        healing_keywords = [
            "daily life", "living with", "self care", "exercise",
            "meditation", "art", "creative", "hobby", "nature",
            "healing", "wellness", "mindfulness", "gratitude",
        ]

        try:
            url = "https://www.reddit.com/r/neurofibromatosis/new.json?limit=50"
            data = json.loads(http_get(url))
            posts = data.get("data", {}).get("children", [])

            for post in posts:
                pd = post.get("data", {})
                title = pd.get("title", "").lower()
                body = pd.get("selftext", "").lower()
                full_text = f"{title} {body}"

                # ë¶€ì •ì  ì½˜í…ì¸  í•„í„°ë§
                if any(neg in full_text for neg in NEGATIVE_KEYWORDS):
                    continue

                # íë§ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­
                healing_score = sum(1 for kw in healing_keywords if kw in full_text)
                if healing_score > 0:
                    items.append({
                        "source": "reddit_healing",
                        "title": pd.get("title", ""),
                        "body": pd.get("selftext", "")[:500],
                        "healing_score": healing_score,
                        "url": f"https://reddit.com{pd.get('permalink', '')}",
                        "fetched_at": datetime.now().isoformat(),
                    })

            items.sort(key=lambda x: x["healing_score"], reverse=True)
            print(f"  â†’ íë§ ê²Œì‹œë¬¼ {len(items)}ê±´ ìˆ˜ì§‘")

        except Exception as e:
            print(f"  âŒ íë§ ì½˜í…ì¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return items

    def fetch_all(self) -> dict:
        """ëª¨ë“  í™˜ì ì´ì•¼ê¸° ì†ŒìŠ¤ ìˆ˜ì§‘"""
        return {
            "reddit_stories": self.fetch_reddit(),
            "ctf_stories": self.fetch_ctf_stories(),
            "ctf_news": self.fetch_ctf_news_stories(),
            "healing": self.fetch_healing_content(),
        }


def save_results(data, filename: str):
    output_dir = os.path.join(os.path.dirname(__file__), "data")
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath


def main():
    parser = argparse.ArgumentParser(description="END NF í™˜ì ì´ì•¼ê¸° ìˆ˜ì§‘ê¸°")
    parser.add_argument("--source", type=str, default="all",
                        help="ìˆ˜ì§‘ ì†ŒìŠ¤ (reddit, ctf, healing, all)")
    parser.add_argument("--max", type=int, default=20, help="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--output", type=str, default="", help="ì¶œë ¥ íŒŒì¼ëª…")
    args = parser.parse_args()

    fetcher = PatientStoryFetcher()

    if args.source == "reddit":
        results = fetcher.fetch_reddit(args.max)
    elif args.source == "ctf":
        results = {
            "stories": fetcher.fetch_ctf_stories(),
            "news": fetcher.fetch_ctf_news_stories(),
        }
    elif args.source == "healing":
        results = fetcher.fetch_healing_content()
    else:
        results = fetcher.fetch_all()

    filename = args.output or f"patient_stories_{datetime.now().strftime('%Y%m%d')}.json"
    save_results(results, filename)


if __name__ == "__main__":
    main()

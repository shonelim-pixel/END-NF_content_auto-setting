"""
============================================================
END NF ì½˜í…ì¸  ì‹œìŠ¤í…œ - PubMed ë…¼ë¬¸ ìˆ˜ì§‘ê¸°
============================================================
PubMed E-utilities APIë¥¼ ì‚¬ìš©í•˜ì—¬ NF ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python pubmed_fetcher.py
    python pubmed_fetcher.py --days 7 --max 20
    python pubmed_fetcher.py --query "selumetinib NF1" --output results.json

í™˜ê²½ë³€ìˆ˜:
    NCBI_API_KEY: NCBI API í‚¤ (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ 3req/sec ì œí•œ)
"""

import os
import json
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from urllib.parse import quote
from typing import Optional
import argparse

# â”€â”€ ì§ì ‘ HTTP ìš”ì²­ (requests ì—†ì´ë„ ë™ì‘) â”€â”€
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    import urllib.request
    import urllib.error
    HAS_REQUESTS = False


def http_get(url: str, timeout: int = 30) -> str:
    """HTTP GET ìš”ì²­ (requests ë˜ëŠ” urllib ì‚¬ìš©)"""
    if HAS_REQUESTS:
        resp = requests.get(url, timeout=timeout)
        resp.raise_for_status()
        return resp.text
    else:
        req = urllib.request.Request(url, headers={"User-Agent": "ENDNF-ContentBot/1.0"})
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            return resp.read().decode("utf-8")


# â”€â”€ ì„¤ì • â”€â”€
BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
API_KEY = os.environ.get("NCBI_API_KEY", "")

DEFAULT_QUERIES = [
    "neurofibromatosis",
    "neurofibromatosis type 1 treatment",
    "NF1 plexiform neurofibroma",
    "selumetinib neurofibromatosis",
    "schwannomatosis therapy",
    "MEK inhibitor NF1",
    "neurofibromatosis gene therapy",
]

PROFESSOR_LEE_QUERIES = [
    "Lee Beom Hee[Author] AND genetics",
    "Lee BH[Author] AND Seoul National University AND rare disease",
    "Lee Beom Hee[Author] AND neurofibromatosis",
]


class PubMedFetcher:
    """PubMed E-utilitiesë¥¼ í†µí•œ NF ë…¼ë¬¸ ìˆ˜ì§‘ê¸°"""

    def __init__(self, api_key: str = ""):
        self.api_key = api_key or API_KEY
        self.delay = 0.1 if self.api_key else 0.34  # APIí‚¤ ìˆìœ¼ë©´ 10req/s, ì—†ìœ¼ë©´ 3req/s

    def _build_params(self, params: dict) -> str:
        """URL íŒŒë¼ë¯¸í„° ë¹Œë“œ"""
        if self.api_key:
            params["api_key"] = self.api_key
        params["tool"] = "ENDNF_ContentBot"
        params["email"] = "endnf.content@gmail.com"
        return "&".join(f"{k}={quote(str(v))}" for k, v in params.items())

    def search(self, query: str, max_results: int = 10, days_back: int = 30) -> list:
        """
        PubMed ê²€ìƒ‰ í›„ PMID ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        Args:
            query: ê²€ìƒ‰ì–´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            days_back: ìµœê·¼ Nì¼ ì´ë‚´ ë…¼ë¬¸ë§Œ

        Returns:
            PMID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        date_from = (datetime.now() - timedelta(days=days_back)).strftime("%Y/%m/%d")
        date_to = datetime.now().strftime("%Y/%m/%d")

        params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "sort": "date",
            "datetype": "pdat",
            "mindate": date_from,
            "maxdate": date_to,
            "retmode": "json",
        }

        url = f"{BASE_URL}/esearch.fcgi?{self._build_params(params)}"
        print(f"  ğŸ” ê²€ìƒ‰: '{query}' (ìµœê·¼ {days_back}ì¼)")

        try:
            data = json.loads(http_get(url))
            id_list = data.get("esearchresult", {}).get("idlist", [])
            count = data.get("esearchresult", {}).get("count", "0")
            print(f"     â†’ ì´ {count}ê±´ ì¤‘ {len(id_list)}ê±´ ìˆ˜ì§‘")
            time.sleep(self.delay)
            return id_list
        except Exception as e:
            print(f"     âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def fetch_details(self, pmids: list) -> list:
        """
        PMID ë¦¬ìŠ¤íŠ¸ë¡œ ë…¼ë¬¸ ìƒì„¸ì •ë³´ ì¡°íšŒ

        Args:
            pmids: PMID ë¦¬ìŠ¤íŠ¸

        Returns:
            ë…¼ë¬¸ ìƒì„¸ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not pmids:
            return []

        params = {
            "db": "pubmed",
            "id": ",".join(pmids),
            "retmode": "xml",
            "rettype": "abstract",
        }

        url = f"{BASE_URL}/efetch.fcgi?{self._build_params(params)}"
        print(f"  ğŸ“„ {len(pmids)}ê±´ ìƒì„¸ì •ë³´ ì¡°íšŒ ì¤‘...")

        try:
            xml_text = http_get(url)
            time.sleep(self.delay)
            return self._parse_articles(xml_text)
        except Exception as e:
            print(f"     âŒ ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def _parse_articles(self, xml_text: str) -> list:
        """XML íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ"""
        articles = []
        try:
            root = ET.fromstring(xml_text)
        except ET.ParseError as e:
            print(f"     âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

        for article in root.findall(".//PubmedArticle"):
            try:
                medline = article.find(".//MedlineCitation")
                art = medline.find(".//Article")

                # PMID
                pmid = medline.findtext(".//PMID", "")

                # ì œëª©
                title = art.findtext(".//ArticleTitle", "ì œëª© ì—†ìŒ")

                # ì´ˆë¡
                abstract_parts = []
                for abs_text in art.findall(".//Abstract/AbstractText"):
                    label = abs_text.get("Label", "")
                    text = abs_text.text or ""
                    # íƒœê·¸ ë‚´ë¶€ í…ìŠ¤íŠ¸ í¬í•¨
                    full_text = ET.tostring(abs_text, encoding="unicode", method="text").strip()
                    if label:
                        abstract_parts.append(f"[{label}] {full_text}")
                    else:
                        abstract_parts.append(full_text)
                abstract = " ".join(abstract_parts) if abstract_parts else "ì´ˆë¡ ì—†ìŒ"

                # ì €ì
                authors = []
                for author in art.findall(".//AuthorList/Author"):
                    last = author.findtext("LastName", "")
                    first = author.findtext("ForeName", "")
                    if last:
                        authors.append(f"{last} {first}".strip())

                # ì €ë„
                journal = art.findtext(".//Journal/Title", "")
                journal_abbrev = art.findtext(".//Journal/ISOAbbreviation", "")

                # ì¶œíŒì¼
                pub_date_elem = art.find(".//Journal/JournalIssue/PubDate")
                if pub_date_elem is not None:
                    year = pub_date_elem.findtext("Year", "")
                    month = pub_date_elem.findtext("Month", "")
                    day = pub_date_elem.findtext("Day", "")
                    pub_date = f"{year} {month} {day}".strip()
                else:
                    pub_date = ""

                # DOI
                doi = ""
                for eid in article.findall(".//PubmedData/ArticleIdList/ArticleId"):
                    if eid.get("IdType") == "doi":
                        doi = eid.text or ""
                        break

                # í‚¤ì›Œë“œ
                keywords = []
                for kw in medline.findall(".//KeywordList/Keyword"):
                    if kw.text:
                        keywords.append(kw.text)

                # MeSH ìš©ì–´
                mesh_terms = []
                for mesh in medline.findall(".//MeshHeadingList/MeshHeading/DescriptorName"):
                    if mesh.text:
                        mesh_terms.append(mesh.text)

                articles.append({
                    "pmid": pmid,
                    "title": title,
                    "abstract": abstract[:1000],  # ê¸¸ì´ ì œí•œ
                    "authors": authors[:5],  # ì²˜ìŒ 5ëª…
                    "author_count": len(authors),
                    "journal": journal,
                    "journal_abbrev": journal_abbrev,
                    "pub_date": pub_date,
                    "doi": doi,
                    "keywords": keywords[:10],
                    "mesh_terms": mesh_terms[:10],
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    "fetched_at": datetime.now().isoformat(),
                })

            except Exception as e:
                print(f"     âš ï¸ ë…¼ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        print(f"     âœ… {len(articles)}ê±´ íŒŒì‹± ì™„ë£Œ")
        return articles

    def search_and_fetch(self, query: str, max_results: int = 10, days_back: int = 30) -> list:
        """ê²€ìƒ‰ + ìƒì„¸ì •ë³´ ì¡°íšŒë¥¼ í•œ ë²ˆì—"""
        pmids = self.search(query, max_results, days_back)
        if not pmids:
            return []
        return self.fetch_details(pmids)

    def fetch_nf_latest(self, days_back: int = 30, max_per_query: int = 5) -> dict:
        """
        ëª¨ë“  NF ê´€ë ¨ ì¿¼ë¦¬ë¡œ ìµœì‹  ë…¼ë¬¸ ìˆ˜ì§‘

        Returns:
            {query: [articles]} ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 60)
        print("ğŸ“š PubMed NF ìµœì‹  ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘")
        print(f"   ê¸°ê°„: ìµœê·¼ {days_back}ì¼")
        print("=" * 60)

        results = {}
        seen_pmids = set()

        for query in DEFAULT_QUERIES:
            articles = self.search_and_fetch(query, max_per_query, days_back)
            # ì¤‘ë³µ ì œê±°
            unique = []
            for art in articles:
                if art["pmid"] not in seen_pmids:
                    seen_pmids.add(art["pmid"])
                    unique.append(art)
            results[query] = unique
            print(f"     (ê³ ìœ  ë…¼ë¬¸: {len(unique)}ê±´)")
            print()

        total = sum(len(v) for v in results.values())
        print(f"âœ… ì´ {total}ê±´ì˜ ê³ ìœ  ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        return results

    def fetch_professor_lee(self, days_back: int = 365) -> list:
        """ì´ë²”í¬ êµìˆ˜ë‹˜ ê´€ë ¨ ë…¼ë¬¸ ìˆ˜ì§‘"""
        print("\n" + "=" * 60)
        print("ğŸ¥ ì´ë²”í¬ êµìˆ˜ë‹˜ ê´€ë ¨ ë…¼ë¬¸ ìˆ˜ì§‘")
        print("=" * 60)

        all_articles = []
        seen_pmids = set()

        for query in PROFESSOR_LEE_QUERIES:
            articles = self.search_and_fetch(query, 10, days_back)
            for art in articles:
                if art["pmid"] not in seen_pmids:
                    seen_pmids.add(art["pmid"])
                    all_articles.append(art)
            print()

        print(f"âœ… ì´ë²”í¬ êµìˆ˜ë‹˜ ê´€ë ¨ {len(all_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles


def save_results(data: dict | list, filename: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = os.path.join(os.path.dirname(__file__), "data")
    os.makedirs(output_dir, exist_ok=True)

    filepath = os.path.join(output_dir, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath


def main():
    parser = argparse.ArgumentParser(description="END NF PubMed ë…¼ë¬¸ ìˆ˜ì§‘ê¸°")
    parser.add_argument("--days", type=int, default=30, help="ìµœê·¼ Nì¼ ì´ë‚´ (ê¸°ë³¸: 30)")
    parser.add_argument("--max", type=int, default=5, help="ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 5)")
    parser.add_argument("--query", type=str, default="", help="íŠ¹ì • ê²€ìƒ‰ì–´ (ë¯¸ì§€ì •ì‹œ ì „ì²´ ê²€ìƒ‰)")
    parser.add_argument("--professor", action="store_true", help="ì´ë²”í¬ êµìˆ˜ë‹˜ ë…¼ë¬¸ ìˆ˜ì§‘")
    parser.add_argument("--output", type=str, default="", help="ì¶œë ¥ íŒŒì¼ëª…")
    args = parser.parse_args()

    fetcher = PubMedFetcher()

    if args.professor:
        results = fetcher.fetch_professor_lee(args.days)
        filename = args.output or f"professor_lee_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)

    elif args.query:
        results = fetcher.search_and_fetch(args.query, args.max, args.days)
        filename = args.output or f"pubmed_custom_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)

    else:
        results = fetcher.fetch_nf_latest(args.days, args.max)
        filename = args.output or f"pubmed_nf_{datetime.now().strftime('%Y%m%d')}.json"
        save_results(results, filename)


if __name__ == "__main__":
    main()
